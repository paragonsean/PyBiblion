{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "from requests import Session\n",
    "from furnace.semantic_scholar_paper import request_query,S2paper\n",
    "# doc = fetch_semanticscholar(\"arXiv:1705.10311\")\n",
    "rq = request_query('CLIP distillation',0,10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T01:07:15.163245100Z",
     "start_time": "2023-10-24T01:07:14.981959100Z"
    }
   },
   "id": "a8ccd1ae68b5b31b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all:\"CLIP\" AND all:\"saf\" AND all:\"asf\"\n"
     ]
    }
   ],
   "source": [
    "query_words='CLIP saf asf'\n",
    "words = query_words.split(' ')\n",
    "words = [f'all:\"{word}\"' for word in words]\n",
    "\n",
    "# query = f'all:\"infrared\" AND all:\"small\" AND all:\"Target detection\"'\n",
    "query = ' AND '.join(words)\n",
    "print(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T01:21:31.974037200Z",
     "start_time": "2023-10-24T01:21:31.730087Z"
    }
   },
   "id": "cee1aeb04dcb9663"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinyclip: clip distillation via affinity mimicking and weight inheritance\n",
      "enabling multimodal generation on clip via vision-language knowledge distillation\n",
      "clip-td: clip targeted distillation for vision-language tasks\n",
      "grow-and-clip: informative-yet-concise evidence distillation for answer explanation\n",
      "improving clip robustness with knowledge distillation and self-training\n",
      "symmetrical linguistic feature distillation with clip for scene text recognition\n",
      "exploiting clip for zero-shot hoi detection requires knowledge distillation at multiple levels\n",
      "local 3d editing via 3d distillation of clip knowledge\n",
      "decomposing nerf for editing via feature field distillation\n",
      "contrastive learning rivals masked image modeling in fine-tuning via feature distillation\n"
     ]
    }
   ],
   "source": [
    "for paper in rq['data']:\n",
    "    # print(paper)\n",
    "    s2p = S2paper(paper,ref_type='entity',filled_authors=False)\n",
    "    print(s2p.title)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T01:07:25.155953900Z",
     "start_time": "2023-10-24T01:07:25.085471300Z"
    }
   },
   "id": "d421b9e3c8b57549"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published at 2023-04-05 00:00:00, the paper \"segment anything\" has garnered 1392 citations. Authored by 12 researchers, it is now accepted by the IEEE International Conference on Computer Vision.\n"
     ]
    }
   ],
   "source": [
    "from furnace.semantic_scholar_paper import *\n",
    "paper_seg_anything = S2paper(ref_obj='Segment anything') # ‚Üê Input paper title here\n",
    "print(f'Published at {paper_seg_anything.publication_date}, the paper \"{paper_seg_anything.title}\" has garnered {paper_seg_anything.citation_count} citations. Authored by {len(paper_seg_anything.authors)} researchers, it is now accepted by the {paper_seg_anything.publication_source}.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T05:06:27.051417100Z",
     "start_time": "2024-01-27T05:06:27.007938900Z"
    }
   },
   "id": "5c42ba71ae91b117"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RAW Demo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69fd44a7b9b0adb"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"offset\": 0,\n",
      "  \"next\": 100,\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \", detection and segmentation, using YOLOv7 [48] and SAM [49], respectively.\",\n",
      "        \"6) Effects on Downstream Tasks: We study the effects of our method on robotic applications, e.g., detection and segmentation, using YOLOv7 [48] and SAM [49], respectively.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"89ea771d4ba9765aa03863f58a827d2d52be0fdc\",\n",
      "        \"title\": \"PSNet: Towards Efficient Image Restoration With Self-Attention\",\n",
      "        \"venue\": \"IEEE Robotics and Automation Letters\",\n",
      "        \"referenceCount\": 49,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-09-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2042696786\",\n",
      "            \"name\": \"Yuning Cui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2198326677\",\n",
      "            \"name\": \"Alois Knoll\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"In the SAM paper, the authors compare SAM with RITM, FocalClick, and SimpleClick, which are mentioned in this article [26].\",\n",
      "        \"released the Segment Anything Model (SAM) capable of accurately segmenting any object within an image using just a single click, and built the largest segmentation dataset to date [26].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"bfafe3d0b7423029fdfdc7e465fad939e7e5b840\",\n",
      "        \"title\": \"Interactive segmentation in aerial images: a new benchmark and an open access web-based tool\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 42,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Zhe Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2164249365\",\n",
      "            \"name\": \"Shoukun Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10661405\",\n",
      "            \"name\": \"Xiang Que\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2140407137\",\n",
      "            \"name\": \"Xiaogang Ma\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"To retrain or fine-tune SAM on Nuclei datasets, 256 A100 GPUS are required in the training phase [10].\",\n",
      "        \"The prompt encoder and mask decoder of SPPNet reuse the pre-trained weight of SAM [10].\",\n",
      "        \"Recently, segment anything model (SAM) [10] was proposed as a foundation model for natural image segmentation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"5eb5bdccef13fa54263c301389a87937ecefa751\",\n",
      "        \"title\": \"SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 19,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2118219995\",\n",
      "            \"name\": \"Qing Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9551189\",\n",
      "            \"name\": \"Wenwei Kuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160067766\",\n",
      "            \"name\": \"Zeyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233087025\",\n",
      "            \"name\": \"Xueyao Bao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Haoran Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112421696\",\n",
      "            \"name\": \"Wenting Duan\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"SAM (Segment Anything Model) is a recent foundation model for segmentation [6, 35].\",\n",
      "        \"However, unleashing the full potential of deep neural networks requires vast amounts of data [7, 35, 86].\",\n",
      "        \"To showcase the complexity of our TYC dataset, we report qualitative segmentation results of the Segment Anything Model (SAM), a recent foundation model for segmentation [35].\",\n",
      "        \"Subsequently, the widespread availability of large-scale public datasets, such as ImageNet [69], KITTI [25], Cityscapes [13], Ego4D [26], SA-1B [35], and LAION-5B [73], has been a major contributor to the recent success of deep neural network approaches.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"b3fd0cdbff54c6a8fc2b1df9989ccd801d0b5eef\",\n",
      "        \"title\": \"The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 90,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2060445879\",\n",
      "            \"name\": \"Christoph Reich\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51245492\",\n",
      "            \"name\": \"Tim Prangemeier\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1716763\",\n",
      "            \"name\": \"H. Koeppl\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"8f3fe1ad8ad942bddcbcb55df0842b7a0f318783\",\n",
      "        \"title\": \"HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 26,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2143435307\",\n",
      "            \"name\": \"Jonathan Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2191737644\",\n",
      "            \"name\": \"Amna Elmustafa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2005230796\",\n",
      "            \"name\": \"L. Weldegebriel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"97717104\",\n",
      "            \"name\": \"Emnet Negash\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233245523\",\n",
      "            \"name\": \"Richard Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"83262128\",\n",
      "            \"name\": \"Chenlin Meng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2490652\",\n",
      "            \"name\": \"Stefano Ermon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2465182\",\n",
      "            \"name\": \"D. Lobell\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"82e29cc0a07e25998021c9f9af426cae11a62953\",\n",
      "        \"title\": \"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 42,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"11007025\",\n",
      "            \"name\": \"Junjiao Tian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7944702\",\n",
      "            \"name\": \"Lavisha Aggarwal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1979935\",\n",
      "            \"name\": \"Andrea Colaco\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145276578\",\n",
      "            \"name\": \"Z. Kira\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1403064530\",\n",
      "            \"name\": \"Mar Gonz\\u00e1lez-Franco\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"f4b1032b6e1fce339502b4694727a078347bcbb3\",\n",
      "        \"title\": \"Implicit encoding via semantic redundancy elimination enabling adaptive compression of multidimensional biological data\",\n",
      "        \"venue\": \"bioRxiv\",\n",
      "        \"referenceCount\": 35,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2233791882\",\n",
      "            \"name\": \"Yifan Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"19235013\",\n",
      "            \"name\": \"Chengqiang Yi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2141888757\",\n",
      "            \"name\": \"Yao Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233828902\",\n",
      "            \"name\": \"Shimeng Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233996194\",\n",
      "            \"name\": \"Jianchao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2234006676\",\n",
      "            \"name\": \"Binbing Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50523576\",\n",
      "            \"name\": \"Peng Fei\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"(Normal, Dense\\u2026)\\nVisual Question Answering: (BLIP, Coca,...)\\nVisual Dialog: (LLaVa, LLaMaAdapter, Mini-GPT4\\u2026)\\nX-Anything (SAM, SEEM\\u2026)\\nImage Description\\nTopic LLM APIs\\nCamera Node\\nGPT Consultation\\nNode\\nGPT Consultation\\nTopic\\nVisual PromptLLM Prompt\\nFig.\",\n",
      "        \"The task of SAM is to return a valid segmentation mask given any segmentation prompt.\",\n",
      "        \"In the final stage, SAM is prompted with a regular grid of foreground points, yielding an average of around 100 high-quality masks per image.\",\n",
      "        \"\\u2013 GPT_temperature: 0.2 \\u2022 MiniGPT4_parameters:\\n\\u2013 configuration:\\u201dminigpt4 eval.yaml\\u201d # comment: absolute path for configuration of MiniGPT4 model\\n\\u2013 temperature_miniGPT4: 0.2 \\u2022 llava_parameters:\\n\\u2013 temperature_llavA: 0.2 \\u2013 llama_version:\\u201d13B\\u201d # comment: you can\\nchoose between [7B, 13B] \\u2022 SAM_parameters:\\n\\u2013 weights_SAM:\\u201dsam vit h 4b8939.pth\\u201d # comment: absolute path for configuration of MiniGPT4 mode\",\n",
      "        \"\\u2022 SAM_parameters: This section contains the configuration for the SAM model.\",\n",
      "        \"In the second stage, SAM can automatically generate masks for a subset of objects, and annotators focus on annotating the remaining objects.\",\n",
      "        \"It assembles many predefined methods for this topic, such as LLAVA model [35], MiniGPT-4 [33], and SAM (Segment Anything) [41]:\\n1) LLaVA Model The LLaVA (Large Language-and-Vision Assistant) model represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding [35].\",\n",
      "        \"\\u2013 weights_SAM: This field specifies the weights used by the SAM model.\",\n",
      "        \"The Segment Anything Model (SAM) [41] is a foundational model for image segmentation that is designed to be promptable, meaning it can be guided to perform specific tasks via prompts.\",\n",
      "        \"It integrates advanced VLM models such as LLAVA, MiniGPT-4, and SAM.\",\n",
      "        \"In the first stage, SAM assists annotators in annotating masks.\",\n",
      "        \"SAM consists of three main components: an image encoder that computes an image embedding, a prompt encoder that embeds prompts, and a lightweight mask decoder that predicts segmentation masks based on the image and prompt embeddings.\",\n",
      "        \"Presented below is the YAML configuration file used for CarMate: \\u2022 Task_name: driver phone usage \\u2022 ROSGPT_Vision_Camera_Node:\\n\\u2013 Image_Description_Method: llava # comment: you can choose between [llava, MiniGPT4, SAM] \\u2013 Vision_prompt:\\u201dDescribe the driver\\u2019s current level of focus on driving based on the visual cues, Answer with one short sentence.\\u201d\",\n",
      "        \"To train SAM, a diverse, large-scale source of data is needed.\",\n",
      "        \"3) The Segment Anything Model (SAM) model The Segment Anything Model (SAM) [41] is a foundational model for image segmentation that is designed to be promptable, meaning it can be guided to perform specific tasks via prompts.\",\n",
      "        \"It can be one of the currently developed methods: MiniGPT4, LLaVA, or SAM.\",\n",
      "        \"It assembles many predefined methods for this topic, such as LLAVA model [35], MiniGPT-4 [33], and SAM (Segment Anything) [41]:\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"53e8d327e7ceda6f4efd321752da57edbaee6257\",\n",
      "        \"title\": \"ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 44,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"11042085\",\n",
      "            \"name\": \"Bilel Benjdira\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1714415\",\n",
      "            \"name\": \"A. Koub\\u00e2a\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107609407\",\n",
      "            \"name\": \"Anas M. Ali\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"like SAM [2] and UniDetector [3] have demonstrated capabilities in open-world object recognition and segmentation.\",\n",
      "        \"If none, delete this.\\nlike SAM [2] and UniDetector [3] have demonstrated capabilities in open-world object recognition and segmentation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"d4346bafff43d0a942a243c957195cedff80c274\",\n",
      "        \"title\": \"Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 24,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2155450576\",\n",
      "            \"name\": \"Zengxiang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153615411\",\n",
      "            \"name\": \"Zhaoxiang Hou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146672712\",\n",
      "            \"name\": \"Hui Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155517674\",\n",
      "            \"name\": \"Ying Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115465812\",\n",
      "            \"name\": \"Tongzhi Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2172756461\",\n",
      "            \"name\": \"Long-Xiang Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233054393\",\n",
      "            \"name\": \"Chao Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2051970229\",\n",
      "            \"name\": \"Che-Sheng Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108238375\",\n",
      "            \"name\": \"Weishan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7769339\",\n",
      "            \"name\": \"Zelei Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109336806\",\n",
      "            \"name\": \"Liang Xu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"f4e26ad29143332c478280bbfec5a2a23b282c5d\",\n",
      "        \"title\": \"Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 58,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2110422360\",\n",
      "            \"name\": \"Jiantao Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2066123456\",\n",
      "            \"name\": \"Shentong Mo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144987292\",\n",
      "            \"name\": \"Muhammad Awais\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"22692781\",\n",
      "            \"name\": \"Sara Atito\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2976854\",\n",
      "            \"name\": \"Zhenhua Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145801638\",\n",
      "            \"name\": \"J. Kittler\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"6eaf524a64de7c079eaa7a10621b0182523ec5e4\",\n",
      "        \"title\": \"SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF)\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 12,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1734758447\",\n",
      "            \"name\": \"Ange Lou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233233055\",\n",
      "            \"name\": \"Yamin Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152174257\",\n",
      "            \"name\": \"X. Yao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1699007\",\n",
      "            \"name\": \"Yike Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2556548\",\n",
      "            \"name\": \"J. Noble\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"8885370112a106e4b9efc9bee9c02b2d889d285f\",\n",
      "        \"title\": \"Segment Anything for Microscopy\",\n",
      "        \"venue\": \"bioRxiv\",\n",
      "        \"referenceCount\": 47,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2212369450\",\n",
      "            \"name\": \"Anwai Archit\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233479685\",\n",
      "            \"name\": \"Sushmita Nair\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2047956859\",\n",
      "            \"name\": \"Nabeel Khalid\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233478907\",\n",
      "            \"name\": \"Paul Hilt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1696614145\",\n",
      "            \"name\": \"Vikas Rajashekar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233479236\",\n",
      "            \"name\": \"Marei Freitag\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233575080\",\n",
      "            \"name\": \"Sagnik Gupta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145279674\",\n",
      "            \"name\": \"A. Dengel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1734717217\",\n",
      "            \"name\": \"Sheraz Ahmed\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8814947\",\n",
      "            \"name\": \"Constantin Pape\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"157d4db9b898e68a68c56858e6a18425fbd128be\",\n",
      "        \"title\": \"Towards Accelerated Model Training via Bayesian Data Selection\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 45,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"145114723\",\n",
      "            \"name\": \"Zhijie Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153522384\",\n",
      "            \"name\": \"Peng Cui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145254043\",\n",
      "            \"name\": \"Jun Zhu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"5 \\u00b7 [f \\u00b7 log(f + \\u03b5) + (1\\u2212 f) \\u00b7 log(1\\u2212 f + \\u03b5)] (2)\"\n",
      "      ],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"0f24ac2cb4ccc5aaeaf69c15a416c37449abe8ca\",\n",
      "        \"title\": \"False Negative/Positive Control for SAM on Noisy Medical Images\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 18,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2115586738\",\n",
      "            \"name\": \"Xing Yao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145508089\",\n",
      "            \"name\": \"Han Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150968696\",\n",
      "            \"name\": \"Dewei Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1703882745\",\n",
      "            \"name\": \"Daiwei Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1734758447\",\n",
      "            \"name\": \"Ange Lou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2156011043\",\n",
      "            \"name\": \"Hao Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48854767\",\n",
      "            \"name\": \"Ruining Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232783423\",\n",
      "            \"name\": \"Gabriel Arenas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"24368180\",\n",
      "            \"name\": \"Baris U. Oguz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2199333392\",\n",
      "            \"name\": \"Nadav Schwartz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31617805\",\n",
      "            \"name\": \"B. Byram\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47712388\",\n",
      "            \"name\": \"I. Oguz\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"733f42314555def0d4983e31c5598162657839e8\",\n",
      "        \"title\": \"ControlCom: Controllable Image Composition using Diffusion Model\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 61,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2156622781\",\n",
      "            \"name\": \"Bo Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2210830546\",\n",
      "            \"name\": \"Yuxuan Duan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158820459\",\n",
      "            \"name\": \"Jun Lan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47547671\",\n",
      "            \"name\": \"Y. Hong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3199167\",\n",
      "            \"name\": \"Huijia Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232805019\",\n",
      "            \"name\": \"Weiqiang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232781010\",\n",
      "            \"name\": \"Li Niu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"e83625f963f7720d5742810f6da6bed87c906d6c\",\n",
      "        \"title\": \"UniAP: Towards Universal Animal Perception in Vision via Few-shot Learning\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 57,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2193476510\",\n",
      "            \"name\": \"Meiqi Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143784044\",\n",
      "            \"name\": \"Zhonghan Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2174436552\",\n",
      "            \"name\": \"Wenhao Chai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232950325\",\n",
      "            \"name\": \"Hanjun Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2205657015\",\n",
      "            \"name\": \"Shidong Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2129508294\",\n",
      "            \"name\": \"Yanting Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145159381\",\n",
      "            \"name\": \"Jenq-Neng Hwang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3385327\",\n",
      "            \"name\": \"Gaoang Wang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"56b5fc7b921f7ff9d3fe99b6fd00849c548dd457\",\n",
      "        \"title\": \"ASPIRE: Language-Guided Augmentation for Robust Image Classification\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 54,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3488077\",\n",
      "            \"name\": \"Sreyan Ghosh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232783850\",\n",
      "            \"name\": \"Chandra Kiran Reddy Evuru\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109551329\",\n",
      "            \"name\": \"Sonal Kumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2051299278\",\n",
      "            \"name\": \"Utkarsh Tyagi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49550738\",\n",
      "            \"name\": \"Sakshi Singh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2147230645\",\n",
      "            \"name\": \"Sanjoy Chowdhury\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2172597446\",\n",
      "            \"name\": \"Dinesh Manocha\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"This changed when Segment Anything Model (SAM) was introduced in April 2023 [17].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"fe28941f520d8993a5c3ddbb1ebc27e799d05e83\",\n",
      "        \"title\": \"SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 30,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"33904534\",\n",
      "            \"name\": \"Botond Fazekas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50228922\",\n",
      "            \"name\": \"Jos\\u00e9 Morano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232607642\",\n",
      "            \"name\": \"Dmitrii Lachinov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"115550376\",\n",
      "            \"name\": \"Guilherme Aresta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2154606862\",\n",
      "            \"name\": \"Hrvoje Bogunovi'c\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Therefore, inspired by the works [36]\\u2013[39], which introduce pre-trained models on large-scale dataset and demonstrate great generalization to a wide range of tasks, we transfer the pre-trained multimodal knowledge including visual and textual modalities from CLIP to our point cloud completion network.\"\n",
      "      ],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"5140757050a85cfb2022ff6ef800951536110a53\",\n",
      "        \"title\": \"Fine-grained Text and Image Guided Point Cloud Completion with CLIP Model\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 71,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2045324909\",\n",
      "            \"name\": \"Wei Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46618464\",\n",
      "            \"name\": \"Jun Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"151474565\",\n",
      "            \"name\": \"Mingjie Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"116740128\",\n",
      "            \"name\": \"Hongchen Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145099446\",\n",
      "            \"name\": \"Nannan Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109302974\",\n",
      "            \"name\": \"Xiuping Liu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"92e503d69a486a16ce26047408b9470f7732009c\",\n",
      "        \"title\": \"MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose and Size Estimation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 36,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2109749941\",\n",
      "            \"name\": \"Jiaqi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35400601\",\n",
      "            \"name\": \"Yucong Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232756539\",\n",
      "            \"name\": \"Xiangting Meng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116550359\",\n",
      "            \"name\": \"C. Yan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39456152\",\n",
      "            \"name\": \"Ming Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1389674940\",\n",
      "            \"name\": \"Ran Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2145533741\",\n",
      "            \"name\": \"Lige Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152305868\",\n",
      "            \"name\": \"Tao Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1727013\",\n",
      "            \"name\": \"L. Kneip\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"The Segment Anything Model (SAM) (Kirillov et al. 2023) has recently gained significant attention as a pioneering foundation model for promptable segmentation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"2cfbeb0c2a0c268773e3b952f022aada9cfd0538\",\n",
      "        \"title\": \"SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 31,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2214120021\",\n",
      "            \"name\": \"Wenxi Yue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Jing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2055735829\",\n",
      "            \"name\": \"Kun Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2190919982\",\n",
      "            \"name\": \"Yong Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116782926\",\n",
      "            \"name\": \"Jiebo Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2184760304\",\n",
      "            \"name\": \"Zhiyong Wang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"Recently, prompt-based interactive semantic segmentation algorithms such as SAM (Kirillov et al. 2023) and SEEM (Zou et al.\",\n",
      "        \"In contrast to interactive semantic segmentation models, such as SAM (Kirillov et al. 2023) and SEEM (Zou et al. 2023), our prompt dataset is generated directly from the ground truth by simulating human prompt habits, including point, box, and mask prompts, without relying on human interaction.\",\n",
      "        \"In contrast to interactive semantic segmentation models, such as SAM (Kirillov et al. 2023) and SEEM (Zou et al.\",\n",
      "        \"Recently, prompt-based interactive semantic segmentation algorithms such as SAM (Kirillov et al. 2023) and SEEM (Zou et al. 2023) have shown promising performance by refining predicted masks through prompt prior information.\",\n",
      "        \"Recently, promptbased interactive semantic segmentation methods, such as SAM (Kirillov et al. 2023) and SEEM (Zou et al.\",\n",
      "        \"Recently, promptbased interactive semantic segmentation methods, such as SAM (Kirillov et al. 2023) and SEEM (Zou et al. 2023), have demonstrated remarkable performance in semantic segmentation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"505d425d41c80c1c040d1411302410bdcacd827c\",\n",
      "        \"title\": \"High-Fidelity Lake Extraction via Two-Stage Prompt Enhancement: Establishing a Novel Baseline and Benchmark\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 29,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2108382749\",\n",
      "            \"name\": \"Ben Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153558948\",\n",
      "            \"name\": \"Xuechao Zou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"94451829\",\n",
      "            \"name\": \"K. Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"41154301\",\n",
      "            \"name\": \"Yu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2067802962\",\n",
      "            \"name\": \"Junliang Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2060157420\",\n",
      "            \"name\": \"Pin Tao\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Many downstream tasks benefit from these models by leveraging prompt engineering [30,6] and fine-tuning techniques [16,29,28] One prominent large foundation model in computer vision, the Segment Anything Model (SAM) [14], is a powerful tool for various segmentation tasks, trained on natural images.\",\n",
      "        \"To introduce zero-shot generalization in segmentation, the team proposed the Segment Anything Model (SAM) [14], which is a large-scaled vision foundation model.\",\n",
      "        \"Recent advancements in large-scale models, such as GPT-4 [20], DALL-E [22], and SAM [14], have shed light on few-shot and even zero-shot learning.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"f119cc048f34f620ad507f7e54e38ccefcb801f9\",\n",
      "        \"title\": \"Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 30,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2167223118\",\n",
      "            \"name\": \"Qi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108307033\",\n",
      "            \"name\": \"Yuyao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158113368\",\n",
      "            \"name\": \"Marawan Elbatel\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"This pipeline ensures a consistent background across the human motion image sequence by leveraging knowledge from the intra-frame alignment module, segment anything (Kirillov et al. 2023), and image inpainting techniques (Rombach et al. 2022 Published).\",\n",
      "        \"Subsequently, employing the segment anything (Kirillov et al. 2023), we derive a body area mask.\",\n",
      "        \"This pipeline ensures a consistent background across the human motion image sequence by leveraging knowledge from the intra-frame alignment module, segment anything (Kirillov et al. 2023), and image inpainting techniques (Rombach et al.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"10603f24428220cb3b78b0c23fb7e24cbee71f95\",\n",
      "        \"title\": \"Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 40,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1740906917\",\n",
      "            \"name\": \"Bosheng Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2218633007\",\n",
      "            \"name\": \"Wentao Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2206456553\",\n",
      "            \"name\": \"Qifan Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118071462\",\n",
      "            \"name\": \"Siliang Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125211\",\n",
      "            \"name\": \"Yueting Zhuang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"13f64a8e8d98e6854a7621eaacfe04364ca66ac0\",\n",
      "        \"title\": \"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 70,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"34271953\",\n",
      "            \"name\": \"Ouyang Hao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2220375698\",\n",
      "            \"name\": \"Qiuyu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2122428116\",\n",
      "            \"name\": \"Yuxi Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2083548117\",\n",
      "            \"name\": \"Qingyan Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118079405\",\n",
      "            \"name\": \"Juntao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"84005711\",\n",
      "            \"name\": \"Kecheng Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145453113\",\n",
      "            \"name\": \"Xiaowei Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2157737759\",\n",
      "            \"name\": \"Qifeng Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2117687899\",\n",
      "            \"name\": \"Yujun Shen\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"Implementation Referring to the robustness evaluation benchmark [7], we have evaluated SAM [14] under 18 types of data corruptions at 5 severity levels following the official implementations(6) with box prompts.\",\n",
      "        \"Because SAM [14] can not provide consistent categorical information.\",\n",
      "        \"The ViT-H-based SAM [14] is employed in all our investigations except for the finetuning experiments.\",\n",
      "        \"Implementation With bounding boxes and single points as prompts, we input the images to SAM [14] to get the predicted binary masks for the target objects.\",\n",
      "        \"For complicated surgical scenes, SAM [14] still struggles to produce accurate segmentation results, as shown in columns (a) to (l) of Fig.\",\n",
      "        \"Results and Analysis The severity of data corruption is directly proportional to the degree of performance degradation in SAM [14], as depicted in Table 2.\",\n",
      "        \"Results and Analysis As shown in Table 1, with bounding boxes as prompts, SAM [14] outperforms previous unprompted supervised methods in binary and\",\n",
      "        \"However, with single points as prompts, SAM [14] degrades a lot in performance, indicating its limited ability to segment surgical instruments from weak prompts.\",\n",
      "        \"The segment anything model (SAM) [14], which has been trained on more than one billion masks, exhibits remarkable proficiency in generating precise object masks using various prompts such as bounding boxes and points.\",\n",
      "        \"Since SAM [14] only predicts binary segmentation masks, for instrument-wise segmentation, the output instrument labels are assigned inherited from the input prompts.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"5e779145a1f40edce3dc01e1be06530776bcf4db\",\n",
      "        \"title\": \"SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 24,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2152824745\",\n",
      "            \"name\": \"An-Chi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145481136\",\n",
      "            \"name\": \"Mobarakol Islam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"92816666\",\n",
      "            \"name\": \"Mengya Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163091268\",\n",
      "            \"name\": \"Yang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1738214662\",\n",
      "            \"name\": \"Hongliang Ren\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"During our experiments, we found that SAM did not work well for very thin objects, for example rice stems, and the problem was resolved only after we added in several images taking in a much closer range.\",\n",
      "        \"We use the Segment-Anything[11] Model (SAM, pre-trained model sam_vit_b_01ec64) to segment the first frame of the video.\",\n",
      "        \"OSTRA uses the Segment-Anything Model (SAM)[11] to identify and extract the object regions of interest in the input image.\",\n",
      "        \"We extend object tracking and 3D reconstruction algorithms to support continuous segmentation labels to leverage the advances in the 2D image segmentation, especially the Segment-Anything Model (SAM) which uses the pretrained neural network without additional training for new scenes, for 3D object segmentation.\",\n",
      "        \"For example, the SAM model released recently by Meta achieves excellent performance in 2D image segmentation tasks[11].\",\n",
      "        \"We used Meta's Segment Anything Model (SAM) for the interactive segmentation.\",\n",
      "        \"SAM takes an image I and a prompt p that specifies a point or bounding box annotation as input, and outputs accurate segmentation masks M as\\n\\ud835\\udc40 = \\ud835\\udc46\\ud835\\udc34\\ud835\\udc40(\\ud835\\udc3c, \\ud835\\udc5d), \\ud835\\udc5d = {\\ud835\\udc5d\\ud835\\udc61\\ud835\\udc52\\ud835\\udc65\\ud835\\udc61 , \\ud835\\udc5d\\ud835\\udc50\\ud835\\udc59\\ud835\\udc56\\ud835\\udc50\\ud835\\udc58}\\nWhere the \\ud835\\udc5d\\ud835\\udc61\\ud835\\udc52\\ud835\\udc65\\ud835\\udc61 is obtained by Grounding-DINO[18] through the input text and the \\ud835\\udc5d\\ud835\\udc50\\ud835\\udc59\\ud835\\udc56\\ud835\\udc50\\ud835\\udc58 is obtained through the user interaction interface (Fig.\",\n",
      "        \"When integrating with SAM 2D segmentation method, our framework essentially eliminates the need for extensive training on 3D scenes, reduces significantly the time, cost and workload of data annotating.\",\n",
      "        \"The first is that it takes advantage of the advances in the 2D image segmentation studies, especially the Segment-Anything Model (SAM) which uses the pretrained neural network without additional training for new scenes, for 3D data segmentation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"6d1eb9d2dcc89b4019e22cc89b2b50584f33bb60\",\n",
      "        \"title\": \"A One Stop 3D Target Reconstruction and multilevel Segmentation Method\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 39,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"40011790\",\n",
      "            \"name\": \"J. Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47748857\",\n",
      "            \"name\": \"Wei Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2046919823\",\n",
      "            \"name\": \"Zhiyan Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2062764121\",\n",
      "            \"name\": \"X. Gan\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"Experiments demonstrate that, compared with SAM [14] and MedSAM [16], CEmb-SAM shows consistent improvements in the segmentation tasks for both peripheral nerves and breast lesions.\",\n",
      "        \"Prompt encoder utilizes user interactions, and mask decoder generates segmentation results based on the image embeddings, prompt embeddings, and its output token [14].\",\n",
      "        \"To evaluate the effectiveness of our method, we compare CEmb-SAM with the U-net [23], SAM [14], and MedSAM [16].\",\n",
      "        \"We leverage recently proposed Segment Anything model (SAM) which has shown great success in natural image segmentation [14].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"5a6b8b8af02b9dd08acca067400c336404a33de5\",\n",
      "        \"title\": \"CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 29,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2220561373\",\n",
      "            \"name\": \"Dongik Shin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3332270\",\n",
      "            \"name\": \"Beomsu Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112407065\",\n",
      "            \"name\": \"Seungjun Baek\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"e55d1013d71c4da14dc5f5d0a22187c80eb8d4f1\",\n",
      "        \"title\": \"Color-NeuS: Reconstructing Neural Implicit Surfaces with Color\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 36,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2197280444\",\n",
      "            \"name\": \"Licheng Zhong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2111809756\",\n",
      "            \"name\": \"Lixin Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2023790905\",\n",
      "            \"name\": \"Kailin Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2226286961\",\n",
      "            \"name\": \"Haoyu Zhen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2229360279\",\n",
      "            \"name\": \"Mei Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1830034\",\n",
      "            \"name\": \"Cewu Lu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"9bf62da1016279354431e2e636bc2d97aea87484\",\n",
      "        \"title\": \"pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 43,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2161175654\",\n",
      "            \"name\": \"Momojit Biswas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2130922939\",\n",
      "            \"name\": \"Himanshu Buckchash\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145052847\",\n",
      "            \"name\": \"Dilip K. Prasad\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Many foundation models, such as CLIP [108], SAM [109], and ChatGPT, have been proposed continuously.\",\n",
      "        \"Therefore, exploring the application of SAM in FSCIOD tasks is also worth investigating.\",\n",
      "        \"Furthermore, the SAM model has potential in object segmentation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"499d6a059c06be0828565388e83c90ef6981181a\",\n",
      "        \"title\": \"Few-shot Class-incremental Learning: A Survey\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 118,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2144131097\",\n",
      "            \"name\": \"Jinghua Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150977987\",\n",
      "            \"name\": \"Li Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1789938\",\n",
      "            \"name\": \"O. Silv\\u00e9n\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145962204\",\n",
      "            \"name\": \"M. Pietik\\u00e4inen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2070035304\",\n",
      "            \"name\": \"Dewen Hu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"9144b4010332136da0584f202db624ce81d1bcba\",\n",
      "        \"title\": \"Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 166,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2109044965\",\n",
      "            \"name\": \"Jiajia Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153557760\",\n",
      "            \"name\": \"Mingle Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40899123\",\n",
      "            \"name\": \"Lirong Xiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158193230\",\n",
      "            \"name\": \"Dong Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51053525\",\n",
      "            \"name\": \"Weichao Zhuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2141036649\",\n",
      "            \"name\": \"Xunyuan Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2177371350\",\n",
      "            \"name\": \"Zhao Li\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"51ccbfce464123cbf2068bc8b07f523c618d9a20\",\n",
      "        \"title\": \"To reverse engineer an entire nervous system\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 91,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3753769\",\n",
      "            \"name\": \"G. Haspel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"16087476\",\n",
      "            \"name\": \"E. Boyden\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2226453870\",\n",
      "            \"name\": \"Jeffrey W. Brown\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2084099785\",\n",
      "            \"name\": \"George M. Church\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145776431\",\n",
      "            \"name\": \"N. Cohen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1397976315\",\n",
      "            \"name\": \"C. Fang-Yen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"14269940\",\n",
      "            \"name\": \"S. Flavell\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2435679\",\n",
      "            \"name\": \"M. Goodman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2891622\",\n",
      "            \"name\": \"A. Hart\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2781726\",\n",
      "            \"name\": \"O. Hobert\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6133776\",\n",
      "            \"name\": \"Konstantinos Kagias\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3286081\",\n",
      "            \"name\": \"S. Lockery\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7774558\",\n",
      "            \"name\": \"Yangning Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2367822\",\n",
      "            \"name\": \"Adam H. Marblestone\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"24023618\",\n",
      "            \"name\": \"Jordan K. Matelsky\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143758236\",\n",
      "            \"name\": \"H. Pfister\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2736829\",\n",
      "            \"name\": \"H. Rotstein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50364955\",\n",
      "            \"name\": \"M. Scholz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2003419\",\n",
      "            \"name\": \"Eli Shlizerman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2231539501\",\n",
      "            \"name\": \"Quilee Simeon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40572235\",\n",
      "            \"name\": \"Vivek Venkatachalam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31930115\",\n",
      "            \"name\": \"G. R. Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"91002032\",\n",
      "            \"name\": \"Ev Yemini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"15271521\",\n",
      "            \"name\": \"M. Zimmer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150174214\",\n",
      "            \"name\": \"K. Kording\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"d5e5b70464988128baf78f7c2aad5195045976e0\",\n",
      "        \"title\": \"TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 23,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2072593695\",\n",
      "            \"name\": \"Shan Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48073628\",\n",
      "            \"name\": \"Qunsheng Ruan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152863039\",\n",
      "            \"name\": \"Qingfeng Wu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Meta released the Segment Anything Model (SAM) [4], a universal segmentation model that utilizes the capabilities of \\u201dprompting\\u201d for image segmentation.\"\n",
      "      ],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"d9e1097a74a75dcc1e5a3d55a30baa9561119ff0\",\n",
      "        \"title\": \"Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 39,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"151471503\",\n",
      "            \"name\": \"Risab Biswas\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"While most extract knowledge from available data, the Segment Anything Model [7] uniquely codevelops alongside model-in-the-loop annotation to construct a custom data engine with over 1 billion masks, conferring strong generalization.\",\n",
      "        \"Index Terms\\u2014Segment Anything Model, Food Recognition, Promptable segmentation, Zero-Shot Segmentation\\nI. INTRODUCTION The landscape of natural language processing [1]\\u2013[3] has been revolutionized by the emergence of large language models [4]\\u2013[6] trained on vast web datasets.\",\n",
      "        \"Revisit of SAM: The Segment Anything Model (SAM) [7] represents the first application of foundation models to the image segmentation task domain.\",\n",
      "        \"SAM demonstrates significant performance on various segmentation benchmarks, showcasing its impressing zero-shot transfer capabilities on 23 diverse segmentation datasets [7].\",\n",
      "        \"the recent unveiling of the Segment Anything Project (SAM) by the Meta AI [7] introduces a groundbreaking promptable segmentation task, which is designed to train a robust vision foundation model.\",\n",
      "        \"The overview of Segment Anything Model (SAM) [7].\",\n",
      "        \"The Segment Anything Model [7] exemplifies this, training\",\n",
      "        \"The Segment Anything Model [7] exemplifies this, training on 11 million images with 1.1 billion masks to segment objects specified in user prompts without additional finetuning.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"1424e2e2369aab3a8986dd09311e18e43ae45612\",\n",
      "        \"title\": \"FoodSAM: Any Food Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 70,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2056436695\",\n",
      "            \"name\": \"Xing Lan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"89582896\",\n",
      "            \"name\": \"Jiayi Lyu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2224083967\",\n",
      "            \"name\": \"Han Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31300562\",\n",
      "            \"name\": \"Kunkun Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2134835603\",\n",
      "            \"name\": \"Zehai Niu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2230521933\",\n",
      "            \"name\": \"Yi Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2067731220\",\n",
      "            \"name\": \"Jian Xue\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"More recently, Segment Anything [10]\\u2014the largest segmentation dataset with more than one billion masks for 11 million images has been released to perform general purpose segmentation tasks.\",\n",
      "        \"More recently, Segment Anything [10]\\u2014the largest segmentation dataset with more than one billion masks\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"872ce5f58259f1d90326762fd078a3af7c516fb4\",\n",
      "        \"title\": \"R2S100K: Road-Region Segmentation Dataset For Semi-Supervised Autonomous Driving in the Wild\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 67,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"40199901\",\n",
      "            \"name\": \"M. A. Butt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2065536481\",\n",
      "            \"name\": \"Hassan Ali\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146174432\",\n",
      "            \"name\": \"Adnan Qayyum\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3195774\",\n",
      "            \"name\": \"Waqas Sultani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1404786833\",\n",
      "            \"name\": \"A. Al-Fuqaha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2139199675\",\n",
      "            \"name\": \"Junaid Qadir\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"We use the current state-of-the-art multimodal pretrained models GPT-4 [27] and Grounded-SAM [22, 21] to produce all the results in this section.\",\n",
      "        \"Figure 7 shows some examples of the Grounded-SAM\\u2019s detection results on images from the Argoverse.\",\n",
      "        \"The robot arm grabs a block and places it outside the table only if the Grounded-SAM returns a confidence score above 0.45 on a red block.\",\n",
      "        \"This operation violates the specification \\u03a6 = \\u00acplace \\u2227 yellow due to the Grounded-SAM misclassifying a yellow block as red.\",\n",
      "        \"We use the Grounded-SAM to evaluate propositions, which lead to the controller\\u2019s state transitions.\",\n",
      "        \"The Grounded-SAM takes an image and a set of propositions in textual form as inputs and classifies which propositions match the image.\",\n",
      "        \"We consider the confidence scores returned by the Grounded-SAM for implementing the control logic.\",\n",
      "        \"Keep in mind that the Grounded-SAM can mistakenly classify objects in the images.\",\n",
      "        \"Vision-language models such as CLIP [18], Yolo [19], and the Segment Anything Model [22] are another type of multimodal pretrained model.\",\n",
      "        \"The Grounded-SAM returns a confidence score for each detected object and the score will be zero if it does not find the object in the image.\",\n",
      "        \"Note that the Grounded-SAM misclassified the red light to the green light.\",\n",
      "        \"Trash/folder_name destination_path\\nWe then ground the controller to the MacOS by using the Grounded-SAM to perceive the task environment, i.e., images of the OS GUI.\",\n",
      "        \"Yolo, R-CNN [20] and Segment Anything Model are object detection models, which take an image and a set of words that describe objects, and classify whether the objects appear in the image.\",\n",
      "        \"Therefore, with the assumption that the Grounded-SAM is reliable, the controller will never perform the cross-road action when the traffic light is not green.\",\n",
      "        \"We collect the confidence scores of the detection results from Grounded-SAM and the corresponding ground truth labels from the dataset.\",\n",
      "        \"We again use the Grounded-SAM as the perception model to ground the controller from Figure 13 to the operating environment.\",\n",
      "        \"We use the current state-of-the-art vision-language model called GroundedSegment-Anything (Grounded-SAM) [22, 21] to evaluate the propositions from image observations.\",\n",
      "        \"We use the Grounded-SAM to evaluate the input symbols and implement the control logic in the real-world task environment.\",\n",
      "        \"Open-vocabulary object detection models [21, 22, 23, 24] remove the constraints on vocabularies, which we will use for connecting the automaton-based representations to the task environment.\",\n",
      "        \"We show how we use the Grounded-SAM to perceive the operating environment and make decisions accordingly.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"56dadb0c70046d1e7fc466d36e05a8bf2bd47307\",\n",
      "        \"title\": \"Multimodal Pretrained Models for Sequential Decision-Making: Synthesis, Verification, Grounding, and Perception\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 38,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1655407483\",\n",
      "            \"name\": \"Yunhao Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1796254983\",\n",
      "            \"name\": \"Cyrus Neary\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3199888\",\n",
      "            \"name\": \"U. Topcu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"0057133a65062fb0530415ab1028d96cbea9d692\",\n",
      "        \"title\": \"From CNN to Transformer: A Review of Medical Image Segmentation Models\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 29,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2210815151\",\n",
      "            \"name\": \"Wenjian Yao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2230298270\",\n",
      "            \"name\": \"Jiajun Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145381133\",\n",
      "            \"name\": \"Wei Liao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1726170149\",\n",
      "            \"name\": \"Yuheng Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107810878\",\n",
      "            \"name\": \"Mengjuan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2187663786\",\n",
      "            \"name\": \"Yao Xie\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"The Segment Anything Model (SAM) [14] is another exemplar of foundation models, recently proposed as a potent tool for image segmentation.\",\n",
      "        \"A particularly compelling instance of these models is the Segment-Anything Model (SAM) [14], which has been trained on an unprecedentedly vast dataset comprising 11 million images and 1 billion masks.\",\n",
      "        \"Foundation models [3, 14, 23] have received significant interests in recent years, owing to their exceptional performance across a multitude of diverse tasks These models typically consume billions of parameters, trained on expansive web-scaled datasets for fundamental tasks such as next token prediction [6] or masked region completion [7].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"5a2c6411456a340fd2e0bf709246a0220902b0b2\",\n",
      "        \"title\": \"Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 41,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2164105032\",\n",
      "            \"name\": \"Rui-Qing Cui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112335557\",\n",
      "            \"name\": \"Siyuan He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2055117257\",\n",
      "            \"name\": \"Shi Qiu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"FAn then leverages foundation models like CLIP [1], DINO [2], and SAM [3] to compute segmentation masks and bounding boxes that best align with the queried objects.\",\n",
      "        \"We use Seg to denote the class-agnostic instance segmentation operator (SAM [3] or Mask2Former [31]).\",\n",
      "        \"we leverage the segment anything model (SAM) [3] for segmentation, DINO [2], and CLIP [1] for general-purpose visual features, and design a lightweight detection and semantic segmentation scheme by combining the features from CLIP and DINO with the class-agnostic instance segmenta-\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"cd50f508d87549ad585aa7250d83a645cf96d0c9\",\n",
      "        \"title\": \"Follow Anything: Open-set detection, tracking, and following in real-time\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 41,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"146119964\",\n",
      "            \"name\": \"Alaa Maalouf\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"103954869\",\n",
      "            \"name\": \"Ninad Jadhav\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7434636\",\n",
      "            \"name\": \"Krishna Murthy Jatavallabhula\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1845901285\",\n",
      "            \"name\": \"Makram Chahine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2231154689\",\n",
      "            \"name\": \"Daniel M.Vogt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2204742847\",\n",
      "            \"name\": \"Robert J. Wood\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143805211\",\n",
      "            \"name\": \"A. Torralba\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064605577\",\n",
      "            \"name\": \"D. Rus\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"3f55c729fede176e90fe17164cbbeb049cdf0480\",\n",
      "        \"title\": \"Pseudo-label Alignment for Semi-supervised Instance Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 55,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"145815850\",\n",
      "            \"name\": \"Jie Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40262099\",\n",
      "            \"name\": \"Cheng Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40626740\",\n",
      "            \"name\": \"Liujuan Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3173509\",\n",
      "            \"name\": \"Shengchuan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2220732888\",\n",
      "            \"name\": \"Annan Shu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2084534028\",\n",
      "            \"name\": \"Guannan Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2192252547\",\n",
      "            \"name\": \"Rongrong Ji\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"8d22808ac913bab4bc5138f8d9f6b5cc072bcf3f\",\n",
      "        \"title\": \"Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 14,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2229279139\",\n",
      "            \"name\": \"Xueyuan Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48854767\",\n",
      "            \"name\": \"Ruining Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46556781\",\n",
      "            \"name\": \"Yucheng Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2142299968\",\n",
      "            \"name\": \"Shunxing Bao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118697332\",\n",
      "            \"name\": \"Haichun Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34430081\",\n",
      "            \"name\": \"Yuankai Huo\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"1636d9de969403087a9c012ed6c0e089fe970c2d\",\n",
      "        \"title\": \"Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 9,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"35933894\",\n",
      "            \"name\": \"Chaoqin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2174865913\",\n",
      "            \"name\": \"Aofan Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46868037\",\n",
      "            \"name\": \"Ya Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108846176\",\n",
      "            \"name\": \"Yanfeng Wang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"The first and most notable segmentation foundation model is the Segment Anything Model (SAM) [7].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"fa948cd629bf195f43c8cb624c3df8ecf6980770\",\n",
      "        \"title\": \"AquaSAM: Underwater Image Foreground Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 33,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2228153927\",\n",
      "            \"name\": \"Muduo Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159468002\",\n",
      "            \"name\": \"Jian-Nan Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108092279\",\n",
      "            \"name\": \"Yutao Liu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"SAM [42] performs image segmentation with various user interactions through prompts.\"\n",
      "      ],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"49e0920069ea17f592395b0bb148c8aac593f6c2\",\n",
      "        \"title\": \"EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 68,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2172427510\",\n",
      "            \"name\": \"Jiajun Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2162733606\",\n",
      "            \"name\": \"Jiacheng Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232106146\",\n",
      "            \"name\": \"Zhiqiang Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6034948\",\n",
      "            \"name\": \"Haolong Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3492011\",\n",
      "            \"name\": \"Ke Nai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8689702\",\n",
      "            \"name\": \"Kailun Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"102745528\",\n",
      "            \"name\": \"Zhiyong Li\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"d026a8fd6032416970d232e67687ab0974065bfa\",\n",
      "        \"title\": \"Harnessing Artificial Intelligence To Reduce Phototoxicity in Live Imaging\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 111,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2210857598\",\n",
      "            \"name\": \"Estibaliz G'omez-de-Mariscal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"89076438\",\n",
      "            \"name\": \"M. Rosario\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2230385402\",\n",
      "            \"name\": \"Joanna W Pylvanainen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3726558\",\n",
      "            \"name\": \"Guillaume Jacquemet\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145601694\",\n",
      "            \"name\": \"Ricardo Henriques\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"d2a0216ecfff5be849c2c9ee389d4a2f3f4aecca\",\n",
      "        \"title\": \"Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 64,\n",
      "        \"citationCount\": 2,\n",
      "        \"publicationDate\": \"2023-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2108998895\",\n",
      "            \"name\": \"Juncheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2212175601\",\n",
      "            \"name\": \"Kaihang Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49931194\",\n",
      "            \"name\": \"Zhiqi Ge\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2211467299\",\n",
      "            \"name\": \"Minghe Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"5462268\",\n",
      "            \"name\": \"Hanwang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2072613978\",\n",
      "            \"name\": \"Wei Ji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108125912\",\n",
      "            \"name\": \"Wenqiao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143779329\",\n",
      "            \"name\": \"Tat-Seng Chua\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118071462\",\n",
      "            \"name\": \"Siliang Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125211\",\n",
      "            \"name\": \"Yueting Zhuang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"193e82306f6b21f0c84cbbe675724ca0b3a44a64\",\n",
      "        \"title\": \"Continual Pre-Training of Large Language Models: How to (re)warm your model?\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 59,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2066789106\",\n",
      "            \"name\": \"Kshitij Gupta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2217687159\",\n",
      "            \"name\": \"Benjamin Th'erien\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2082404155\",\n",
      "            \"name\": \"Adam Ibrahim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"151132591\",\n",
      "            \"name\": \"Mats L. Richter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1404060481\",\n",
      "            \"name\": \"Quentin G. Anthony\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064781956\",\n",
      "            \"name\": \"Eugene Belilovsky\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109771\",\n",
      "            \"name\": \"I. Rish\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"26418330\",\n",
      "            \"name\": \"Timoth\\u00e9e Lesort\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"b201e310d9ba5e5f59709101ba45a848647938f9\",\n",
      "        \"title\": \"Deep learning image segmentation approaches for malignant bone lesions: a systematic review and meta-analysis\",\n",
      "        \"venue\": \"Frontiers in Radiology\",\n",
      "        \"referenceCount\": 90,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2224417057\",\n",
      "            \"name\": \"Joseph M. Rich\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2229793964\",\n",
      "            \"name\": \"Lokesh N. Bhardwaj\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2229067096\",\n",
      "            \"name\": \"Aman Shah\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2231021405\",\n",
      "            \"name\": \"Krish Gangal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2231021419\",\n",
      "            \"name\": \"Mohitha S. Rapaka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2625599\",\n",
      "            \"name\": \"A. Oberai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"5630681\",\n",
      "            \"name\": \"Brandon K.K. Fields\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4259148\",\n",
      "            \"name\": \"G. Matcuk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2691063\",\n",
      "            \"name\": \"V. Duddalwar\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"SAM-ZS uses CLIP to generate prompt embeddings from the given text.\",\n",
      "        \"We also use SAM with text prompt without any training (called Zero Shot performance of SAM or SAM-ZS) [11] for comparison.\",\n",
      "        \"A foundation model called SAM was recently released for the task of promptable segmentation [11].\",\n",
      "        \"We also observe that SAM-ZS does poorly in all the 3 datasets.\",\n",
      "        \"SAM-ZS serves as a baseline as well as an ablation for AdaptiveSAM, thus showing the effectiveness of our method over zero-shot performance of SAM.\",\n",
      "        \"However, CLIP is trained on general datasets and hence, we see that SAM-ZS does not perform well when used as is.\",\n",
      "        \"Similarly, Segment-Anything Model (SAM) was recently released as a foundational model for prompted segmentation [11].\",\n",
      "        \"Segment Anything Model(SAM) [11] proposes a new task of prompted segmentation where given an image and a prompt, the output is a mask specific to the prompt.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"b372709d691121f1695eb2face1e1dcd03a10230\",\n",
      "        \"title\": \"AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 39,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1734834200\",\n",
      "            \"name\": \"Jay N. Paranjape\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2132076159\",\n",
      "            \"name\": \"Nithin Gopalakrishnan Nair\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145654494\",\n",
      "            \"name\": \"S. Sikder\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"32204028\",\n",
      "            \"name\": \"S. Vedula\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2211979791\",\n",
      "            \"name\": \"Vishal M. Patel\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"47ddc365abe54ac7fc5e17ae5be4c9bdc9afec05\",\n",
      "        \"title\": \"Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 37,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2184669155\",\n",
      "            \"name\": \"Akshit Achara\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144228533\",\n",
      "            \"name\": \"R. Pandey\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"We apply SAM [Kirillov et al. 2023] to segment the object from the background for each center-view image.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"e207703fa051ea8e540c59003b83c2ba65aecba7\",\n",
      "        \"title\": \"Learning Photometric Feature Transform for Free-form Object Scan\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 46,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1741521854\",\n",
      "            \"name\": \"Xiang-Wei Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51149666\",\n",
      "            \"name\": \"Kaizhang Kang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2229255638\",\n",
      "            \"name\": \"Fan Pei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228633098\",\n",
      "            \"name\": \"Huakeng Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2227995818\",\n",
      "            \"name\": \"Jinjiang You\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145604260\",\n",
      "            \"name\": \"P. Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144078074\",\n",
      "            \"name\": \"Kun Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1880838\",\n",
      "            \"name\": \"Hongzhi Wu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"We use the off-the-shelf segmentation tool [11] to segment the mirror reflection mask in the image.\",\n",
      "        \"whereM is obtained by using the off-the-shelf segmentation tools like [11] on the ground-truth images.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"85e63afad850ab8750f30d1972c48d130c7025a3\",\n",
      "        \"title\": \"Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 54,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"7216488\",\n",
      "            \"name\": \"Junyi Zeng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159002587\",\n",
      "            \"name\": \"Chong Bao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118229897\",\n",
      "            \"name\": \"Ruiguo Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2541151\",\n",
      "            \"name\": \"Zilong Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"32162658\",\n",
      "            \"name\": \"Guofeng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1679542\",\n",
      "            \"name\": \"H. Bao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1813796\",\n",
      "            \"name\": \"Zhaopeng Cui\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Although recent studies (Kirillov et al. 2023; Han et al. 2022) have achieved impressive success in object segmentation and tracking, it is still difficult to achieve pixel-level accuracy.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"96c430ffce99cdedb6a1b4537113f803e0c3b860\",\n",
      "        \"title\": \"DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 44,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2187856394\",\n",
      "            \"name\": \"Zhongjie Duan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2194540127\",\n",
      "            \"name\": \"Lizhou You\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"121899912\",\n",
      "            \"name\": \"Chengyu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2447610\",\n",
      "            \"name\": \"Cen Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2184067713\",\n",
      "            \"name\": \"Ziheng Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2172016\",\n",
      "            \"name\": \"Weining Qian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2175751063\",\n",
      "            \"name\": \"Jun Huang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"2bd1b8990db73b6495c11082bea2d5f925c5226f\",\n",
      "        \"title\": \"SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 28,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"39541577\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1930128\",\n",
      "            \"name\": \"Nima Tajbakhsh\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Compared to text prompts, visual prompts can be presented in various forms, such as bounding boxes [34], [39], colored blocks [40], positions [41] and points [42], providing greater flexibility to visual prompts.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"228457557d46205c15099397cf507fb3bc9f9f55\",\n",
      "        \"title\": \"E-CLIP: Towards Label-efficient Event-based Open-world Understanding by CLIP\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 67,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2155796995\",\n",
      "            \"name\": \"Jiazhou Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2142380383\",\n",
      "            \"name\": \"Xueye Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2229269938\",\n",
      "            \"name\": \"Yuanhuiyi Lyu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2168616303\",\n",
      "            \"name\": \"Lin Wang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"1caf67dab81d40a23203259ed206b44549b19dbf\",\n",
      "        \"title\": \"Syn-Mediverse: A Multimodal Synthetic Dataset for Intelligent Scene Understanding of Healthcare Facilities\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 42,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"19173320\",\n",
      "            \"name\": \"Rohit Mohan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"104884710\",\n",
      "            \"name\": \"J. Arce\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228846835\",\n",
      "            \"name\": \"Sassan Mokhtar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2036664730\",\n",
      "            \"name\": \"Daniele Cattaneo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2609831\",\n",
      "            \"name\": \"Abhinav Valada\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"illustrate the ability of large vision model [49] to provide comprehensive semantic information, thereby enhancing the agent\\u2019s perception in certain scenes without requiring fine-\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"4855a4ef7429824d7cde4842e3e8402ebc818ca5\",\n",
      "        \"title\": \"A Survey of Object Goal Navigation: Datasets, Metrics and Methods\",\n",
      "        \"venue\": \"2023 IEEE International Conference on Mechatronics and Automation (ICMA)\",\n",
      "        \"referenceCount\": 53,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2233349644\",\n",
      "            \"name\": \"Dewei Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2233374661\",\n",
      "            \"name\": \"Jiaming Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9198155\",\n",
      "            \"name\": \"Jiyu Cheng\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"These point proposals are then used as input for the Segment Anything Model (SAM) [26] to get anomaly instance predictions.\",\n",
      "        \"regularized to avoid predicting objects in unknown regions [39], and the Segment Anything Model (SAM) [26].\",\n",
      "        \"SAM [26] dense + RbA scoring 1024 points \\u2717 73.\",\n",
      "        \"The other end of the spectrum would be to use SAM with the default automatic mask generation module, which samples 32 \\u00d7 32 points in a dense grid and predicts three masks for each point, followed by a series of post-processing steps [26].\",\n",
      "        \"However, we then use the extracted outlier pixels as point guidance for a class-agnostic segmentation pipeline [26] to segment anomalous instances.\",\n",
      "        \"UGainS combines a strong generalist segmentation model (SAM [26]), and a strong out-of-distribution method (RbA [39]).\",\n",
      "        \"Our method uses the recent Segment Anything Model (SAM) [26].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"08f54536409324a014627e5ec8a446fd4f46aad8\",\n",
      "        \"title\": \"UGainS: Uncertainty Guided Anomaly Instance Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 57,\n",
      "        \"citationCount\": 1,\n",
      "        \"publicationDate\": \"2023-08-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"145212578\",\n",
      "            \"name\": \"Alexey Nekrasov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"36665147\",\n",
      "            \"name\": \"Alexander Hermans\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49314527\",\n",
      "            \"name\": \"L. Kuhnert\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064068973\",\n",
      "            \"name\": \"Bastian Leibe\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"We first extract as many regions or instances as possible based on a powerful segmentation model, Segment Anything Model (SAM) (Kirillov et al. 2023).\",\n",
      "        \"SAM is pre-trained based on a large-scale segmentation dataset SA-1B and possesses strong zero-shot generalization ability at part segmentation, which enables us to extract as rich regions as possible.\",\n",
      "        \"In our work, we use SAM to automatically generate object masks for the whole image using a grid of points as the prompt.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"1ee8c8dd9d04247515b33775532b72df7b8ec0f3\",\n",
      "        \"title\": \"RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 43,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2118355549\",\n",
      "            \"name\": \"Qiang Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110961040\",\n",
      "            \"name\": \"Chaohui Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116576240\",\n",
      "            \"name\": \"Shaofeng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51307434\",\n",
      "            \"name\": \"Sitong Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2051262469\",\n",
      "            \"name\": \"Zhibin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49451350\",\n",
      "            \"name\": \"Fan Wang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"659a12d71d8709c132ccd9ccd235f0024cae0239\",\n",
      "        \"title\": \"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 113,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2190474418\",\n",
      "            \"name\": \"Weiyun Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1516268415\",\n",
      "            \"name\": \"Min Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108421389\",\n",
      "            \"name\": \"Qingyun Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47825073\",\n",
      "            \"name\": \"Wen Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1624267274\",\n",
      "            \"name\": \"Zhenhang Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3422833\",\n",
      "            \"name\": \"Linjie Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"66350249\",\n",
      "            \"name\": \"Zhe Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144966716\",\n",
      "            \"name\": \"Hao Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2578924\",\n",
      "            \"name\": \"Xizhou Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2088214652\",\n",
      "            \"name\": \"Zhiguo Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109315186\",\n",
      "            \"name\": \"Yushi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115137018\",\n",
      "            \"name\": \"Tong Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3304536\",\n",
      "            \"name\": \"Jifeng Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145858545\",\n",
      "            \"name\": \"Y. Qiao\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"7b713194fe642e8d99a8d25f6edd41f1ebb513e0\",\n",
      "        \"title\": \"Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 49,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1409858232\",\n",
      "            \"name\": \"Wentong Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49521199\",\n",
      "            \"name\": \"Yu-Jie Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"98828088\",\n",
      "            \"name\": \"Song Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1704030\",\n",
      "            \"name\": \"Jianke Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109020391\",\n",
      "            \"name\": \"Jianshu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150169129\",\n",
      "            \"name\": \"Jian Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Lei Zhang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"6f425ba8c1fe3139fcb886d9dda30cd6520517ac\",\n",
      "        \"title\": \"More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 59,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"49640821\",\n",
      "            \"name\": \"Bang An\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110018762\",\n",
      "            \"name\": \"Sicheng Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1648623943\",\n",
      "            \"name\": \"Michael Panaitescu-Liess\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"29359383\",\n",
      "            \"name\": \"Chaithanya Kumar Mummadi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40070055\",\n",
      "            \"name\": \"Furong Huang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"e8f097dfe1c51f3f4de879b97d93dd78cafb00df\",\n",
      "        \"title\": \"Push the Boundary of SAM: A Pseudo-label Correction Framework for Medical Segmentation\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 51,\n",
      "        \"citationCount\": 1,\n",
      "        \"publicationDate\": \"2023-08-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2116357233\",\n",
      "            \"name\": \"Ziyi Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2052178462\",\n",
      "            \"name\": \"Hongshan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143779155\",\n",
      "            \"name\": \"Haofeng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2082604\",\n",
      "            \"name\": \"F. Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2151013633\",\n",
      "            \"name\": \"A. Laine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47848774\",\n",
      "            \"name\": \"E. Angelini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"38206847\",\n",
      "            \"name\": \"C. Hendon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2067692315\",\n",
      "            \"name\": \"Yu Gan\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"31fecc108ec6923cdae3f658d4e3b6591dacf39e\",\n",
      "        \"title\": \"DiffusePast: Diffusion-based Generative Replay for Class Incremental Semantic Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 49,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2108110907\",\n",
      "            \"name\": \"Jingfan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115829788\",\n",
      "            \"name\": \"Yuxi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49830842\",\n",
      "            \"name\": \"P. Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2157320978\",\n",
      "            \"name\": \"Xiao Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2206162255\",\n",
      "            \"name\": \"Zhaoxiang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153833080\",\n",
      "            \"name\": \"Zhen Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1930238\",\n",
      "            \"name\": \"Qing Li\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"446fb5dead075a1a08862662738f462e9a0e91c8\",\n",
      "        \"title\": \"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 85,\n",
      "        \"citationCount\": 3,\n",
      "        \"publicationDate\": \"2023-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2650640\",\n",
      "            \"name\": \"Cheng-Yu Hsieh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2213831475\",\n",
      "            \"name\": \"Sibei Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2136342754\",\n",
      "            \"name\": \"Chun-Liang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2114175058\",\n",
      "            \"name\": \"Yasuhisa Fujii\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143711421\",\n",
      "            \"name\": \"Alexander J. Ratner\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50521003\",\n",
      "            \"name\": \"Chen-Yu Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145237361\",\n",
      "            \"name\": \"Ranjay Krishna\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1945962\",\n",
      "            \"name\": \"Tomas Pfister\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"c9f49d6372c13250df7f42de20ca1701ec947b71\",\n",
      "        \"title\": \"Identification of Paddy Croplands and Its Stages Using Remote Sensors: A Systematic Review\",\n",
      "        \"venue\": \"Italian National Conference on Sensors\",\n",
      "        \"referenceCount\": 143,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2005901415\",\n",
      "            \"name\": \"Manuel Fern\\u00e1ndez-Urrutia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2595766\",\n",
      "            \"name\": \"M. Arbelo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144806146\",\n",
      "            \"name\": \"A. Gil\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"6a636d7305d0ecc9d8712bf964405e9470efd225\",\n",
      "        \"title\": \"A Multi-Task Fusion Strategy-Based Decision-Making and Planning Method for Autonomous Driving Vehicles\",\n",
      "        \"venue\": \"Italian National Conference on Sensors\",\n",
      "        \"referenceCount\": 28,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2230151878\",\n",
      "            \"name\": \"Weiguo Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8151433\",\n",
      "            \"name\": \"Z. Xiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228998198\",\n",
      "            \"name\": \"Han Fang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2137466633\",\n",
      "            \"name\": \"Ke Huo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1687515\",\n",
      "            \"name\": \"Z. Wang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"99ac567abfe3e7d2e74932ba40e848f0e65d76f1\",\n",
      "        \"title\": \"Machine learning for cross-scale microscopy of viruses\",\n",
      "        \"venue\": \"Cell Reports Methods\",\n",
      "        \"referenceCount\": 184,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"10782270\",\n",
      "            \"name\": \"Anthony Petkidis\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4376350\",\n",
      "            \"name\": \"V. Andriasyan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2437769\",\n",
      "            \"name\": \"U. Greber\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"1941285d30436f8d4a6222d4011489a7aa9ba06e\",\n",
      "        \"title\": \"Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats\",\n",
      "        \"venue\": \"International Joint Conference on Artificial Intelligence\",\n",
      "        \"referenceCount\": 35,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2059470354\",\n",
      "            \"name\": \"Lucia Gordon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2148687825\",\n",
      "            \"name\": \"Nikhil Behari\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2222707344\",\n",
      "            \"name\": \"Samuel Collier\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2213299766\",\n",
      "            \"name\": \"Elizabeth Bondi-Kelly\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"81622595\",\n",
      "            \"name\": \"J. Killian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51198175\",\n",
      "            \"name\": \"Catherine Ressijac\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1662763508\",\n",
      "            \"name\": \"P. Boucher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2223751482\",\n",
      "            \"name\": \"Andrew Davies\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1965937258\",\n",
      "            \"name\": \"M. Tambe\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"We also pass the image pair through a segmentation model like SAM (Kirillov et al., 2023) to get the local\\nimage features since NLVR2 requires us to have an object-level understanding of the image.\",\n",
      "        \"Other than the LLM, even the image encoders such as CLIP and SAM have been pre-trained on very large scale image data and suffer from some of the same issues such as bias, privacy concerns, and openness of the data.\",\n",
      "        \"Thus, we add pre-trained object features from a model such as (Girshick, 2015; Kirillov et al., 2023) to the global feature vector and then follow the same architecture as traditional bridge architectures.\",\n",
      "        \"We tried using CLIP and MAE and our image encoders, SAM as the segmentation model and FLAN-T5 and OPT as our LLMs.\",\n",
      "        \"The failure case of SAM+FLAN+LIMBER is also illustrated in the Table.\",\n",
      "        \"We performed extensive tuning of train batch size, learning rate and number of visual tokens for training our model (SAM + FLAN + LIMBER) but unfortunately, the best we could reach was a performance of 52.69% Showin in Figure 2 is the variation of accuracy of LLaVA with max token length.\",\n",
      "        \"Mrigank Raman Running SAM+FLAN+LIMBER, helping to run LLaVA, writing analysis, future work, and proposed model\\nKousik Running LLaVA, getting and explaining failure modes for the bridge architectures.\",\n",
      "        \"Due to resource constraints, we were unable to pretrain a bridge architecture with SAM and we address it as future work.\",\n",
      "        \"Our contributions can be summarized as follows:\\n\\u2022 We show results on incorporating object level local features from SAM (Kirillov et al., 2023) which is a segmentation model into a vanilla bridge architecture without multimodal pretraining.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"10fab613a758202b001facaea859c9e62a4839bd\",\n",
      "        \"title\": \"Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 54,\n",
      "        \"citationCount\": 1,\n",
      "        \"publicationDate\": \"2023-07-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2163451073\",\n",
      "            \"name\": \"Kousik Rajesh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1838175151\",\n",
      "            \"name\": \"Mrigank Raman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144851671\",\n",
      "            \"name\": \"M. A. Karim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1946040746\",\n",
      "            \"name\": \"Pranit Chawla\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"To be specific, we consider three network architectures from SAM: ViT-B (86M), ViT-L (307M) as well as ViT-H (632M) [81], where the number after each backbone indicates the number of parameters.\",\n",
      "        \"Transferable attack of existing vision foundation models: With recent advancement of vision foundation models such as SAM [30], CLIP [79], ImageBind [80] and etc.\",\n",
      "        \"We perform a preliminary experiment on Segment Anything (SAM) [30] with point prompts and show the result in Fig.\",\n",
      "        \"Lastly, we consider the robustness of vision foundation models [30] w.\",\n",
      "        \"Transferable attack of existing vision foundation models: With recent advancement of vision foundation models such as SAM [30], CLIP [79], ImageBind [80] and etc., more attentions have been paid to the transformer-based segmentation models.\",\n",
      "        \"8: Transferable adversarial attack for Segment Anything [30] with point prompts, where the top row shows predictions of three backbones on the clean RGB image.\"\n",
      "      ],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"74dc649d6535ba3f5190061d8fefe2524093c49c\",\n",
      "        \"title\": \"Transferable Attack for Semantic Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 92,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2161196875\",\n",
      "            \"name\": \"Mengqi He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155698491\",\n",
      "            \"name\": \"Jing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51441199\",\n",
      "            \"name\": \"Zhaoyuan Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40214723\",\n",
      "            \"name\": \"Mingyi He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1712576\",\n",
      "            \"name\": \"N. Barnes\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1681554\",\n",
      "            \"name\": \"Yuchao Dai\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Segment Anything Model (SAM) (Kirillov et al. 2023) is a prompt-based segmentation model.\",\n",
      "        \"Among the large vision models, Segment Anything Model (SAM) (Kirillov et al. 2023) is one of the most suitable for optical flow estimation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"fe37211c32ee4ae6a8fb47ac81e8c24147b9ec99\",\n",
      "        \"title\": \"SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 41,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"5906383\",\n",
      "            \"name\": \"Shili Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118180466\",\n",
      "            \"name\": \"Ruian He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2974027\",\n",
      "            \"name\": \"Weimin Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118138243\",\n",
      "            \"name\": \"Bo Yan\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"38939304bb760473141c2aca0305e44fbe04e6e8\",\n",
      "        \"title\": \"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 92,\n",
      "        \"citationCount\": 7,\n",
      "        \"publicationDate\": \"2023-07-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"118025075\",\n",
      "            \"name\": \"Anthony Brohan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2161343011\",\n",
      "            \"name\": \"Noah Brown\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2196517336\",\n",
      "            \"name\": \"Justice Carbajal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2527420\",\n",
      "            \"name\": \"Yevgen Chebotar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1805203\",\n",
      "            \"name\": \"K. Choromanski\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"95691186\",\n",
      "            \"name\": \"Tianli Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30837327\",\n",
      "            \"name\": \"Danny Driess\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46881670\",\n",
      "            \"name\": \"Chelsea Finn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47686265\",\n",
      "            \"name\": \"Peter R. Florence\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3430433\",\n",
      "            \"name\": \"Chuyuan Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153134021\",\n",
      "            \"name\": \"Montse Gonzalez Arenas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2161342233\",\n",
      "            \"name\": \"K. Gopalakrishnan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2225668575\",\n",
      "            \"name\": \"Kehang Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1944801\",\n",
      "            \"name\": \"Karol Hausman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1505793452\",\n",
      "            \"name\": \"Alexander Herzog\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2726592\",\n",
      "            \"name\": \"Jasmine Hsu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2704814\",\n",
      "            \"name\": \"Brian Ichter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"17818078\",\n",
      "            \"name\": \"A. Irpan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2052368480\",\n",
      "            \"name\": \"Nikhil J. Joshi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144885996\",\n",
      "            \"name\": \"Ryan C. Julian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48313860\",\n",
      "            \"name\": \"Dmitry Kalashnikov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2161342687\",\n",
      "            \"name\": \"Yuheng Kuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2057988112\",\n",
      "            \"name\": \"Isabel Leal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1736651\",\n",
      "            \"name\": \"S. Levine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47407464\",\n",
      "            \"name\": \"H. Michalewski\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2080746\",\n",
      "            \"name\": \"Igor Mordatch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31719101\",\n",
      "            \"name\": \"Karl Pertsch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2251957\",\n",
      "            \"name\": \"Kanishka Rao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163522073\",\n",
      "            \"name\": \"Krista Reymann\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1766489\",\n",
      "            \"name\": \"M. Ryoo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2196524735\",\n",
      "            \"name\": \"Grecia Salazar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2840758\",\n",
      "            \"name\": \"P. Sanketi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3142556\",\n",
      "            \"name\": \"P. Sermanet\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2196040785\",\n",
      "            \"name\": \"Jaspiar Singh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2111007256\",\n",
      "            \"name\": \"Anika Singh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1737285\",\n",
      "            \"name\": \"Radu Soricut\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2195355151\",\n",
      "            \"name\": \"Huong Tran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2657155\",\n",
      "            \"name\": \"Vincent Vanhoucke\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144579461\",\n",
      "            \"name\": \"Q. Vuong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"88728227\",\n",
      "            \"name\": \"Ayzaan Wahid\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"69426588\",\n",
      "            \"name\": \"Stefan Welker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3202367\",\n",
      "            \"name\": \"Paul Wohlhart\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9961095\",\n",
      "            \"name\": \"Ted Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10909315\",\n",
      "            \"name\": \"Tianhe Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2196524598\",\n",
      "            \"name\": \"Brianna Zitkovich\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"443d8b9deae59845a50dea70ec0f452257470fbd\",\n",
      "        \"title\": \"YOLOv8 for Defect Inspection of Hexagonal Directed Self-Assembly Patterns: A Data-Centric Approach\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 31,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1752869429\",\n",
      "            \"name\": \"Enrique Dehaerne\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3259827\",\n",
      "            \"name\": \"Bappaditya Dey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2085001950\",\n",
      "            \"name\": \"Hossein Esfandiar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"14011219\",\n",
      "            \"name\": \"L. Verstraete\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35246264\",\n",
      "            \"name\": \"H. Suh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2529096\",\n",
      "            \"name\": \"S. Halder\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9171909\",\n",
      "            \"name\": \"S. Gendt\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"188e6acd8c45a12ab708e3435e77b4e08c112b18\",\n",
      "        \"title\": \"Fine-Grained 3D Modeling and Semantic Mapping of Coral Reefs Using Photogrammetric Computer Vision and Machine Learning\",\n",
      "        \"venue\": \"Italian National Conference on Sensors\",\n",
      "        \"referenceCount\": 63,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1490719214\",\n",
      "            \"name\": \"J. Zhong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35834541\",\n",
      "            \"name\": \"Ming Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2192661028\",\n",
      "            \"name\": \"Hanqi Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1490728402\",\n",
      "            \"name\": \"J. Qin\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Using this information, we created segmentation masks with the Segment Anything model [18] for the mentioned regions and used these as ground truth.\"\n",
      "      ],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"127de044730da2f418a83ea4e1056bdb20bc0012\",\n",
      "        \"title\": \"Simplified Concrete Dropout - Improving the Generation of Attribution Masks for Fine-grained Classification\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 49,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"49016378\",\n",
      "            \"name\": \"D. Korsch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2068336\",\n",
      "            \"name\": \"M. Shadaydeh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1728382\",\n",
      "            \"name\": \"Joachim Denzler\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"CarPatch3D comprises a set of car patches, selected using the semi-automatic method described\\nabove, along with binary masks generated by the SAM [3], roughly estimated camera parameters, and 3D position and orientation of the vehicles generated by DD3D [2].\",\n",
      "        \"CarPatch3D comprises a set of car patches, selected using the semi-automatic method described above, along with binary masks generated by the SAM [3], roughly estimated camera parameters, and 3D position and orientation of the vehicles generated by DD3D [2].\",\n",
      "        \"2) We filter out images with a low pixel count ratio between the result of the 2D segmentor Segment Anything [3] pixel-level segmentation and the 2D detector\\u2019s result, which indicates high occlusion rate.\",\n",
      "        \"To preprocess the patches, we generate rough camera intrinsics, object instance masks, and 3D detection bounding boxes using a monocular 3D object detector [2] and a pixel-level segmenter [3].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"f86e3f50e83ed134b0b473466609a71b6ed53ed5\",\n",
      "        \"title\": \"Car-Studio: Learning Car Radiance Fields from Single-View and Endless In-the-wild Images\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 51,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1701889\",\n",
      "            \"name\": \"Tianyu Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2176137981\",\n",
      "            \"name\": \"Hao Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144705629\",\n",
      "            \"name\": \"Yang Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153104630\",\n",
      "            \"name\": \"Guyue Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39793900\",\n",
      "            \"name\": \"Ming-Yu Liu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Wu et al. [38] employed adapters to steer the Segment Anything Model (SAM) [23], a promptable ViT-based foundation model trained using 1 billion\\nmasks, to medical image segmentation tasks without updating SAM\\u2019s parameters.\",\n",
      "        \"[38] employed adapters to steer the Segment Anything Model (SAM) [23], a promptable ViT-based foundation model trained using 1 billion\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"5931368ae338ef79006641b945645cb7d4a56176\",\n",
      "        \"title\": \"AViT: Adapting Vision Transformers for Small Skin Lesion Segmentation Datasets\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 42,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"5560370\",\n",
      "            \"name\": \"Siyi Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2318388\",\n",
      "            \"name\": \"Nourhan Bayasi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3049056\",\n",
      "            \"name\": \"G. Hamarneh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51307114\",\n",
      "            \"name\": \"Rafeef Garbi\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"730a68f75b5c647d49fd8b17135d478e32d912bc\",\n",
      "        \"title\": \"Tracking Anything in High Quality\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 42,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2109445463\",\n",
      "            \"name\": \"Jiawen Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1696121\",\n",
      "            \"name\": \"Zhe Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2106575518\",\n",
      "            \"name\": \"Zeqi Hao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2212104433\",\n",
      "            \"name\": \"Shijie Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2156145107\",\n",
      "            \"name\": \"Lu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143987328\",\n",
      "            \"name\": \"Dong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153176123\",\n",
      "            \"name\": \"Huchuan Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1471729863\",\n",
      "            \"name\": \"Bin Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153146760\",\n",
      "            \"name\": \"Ju He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2065438867\",\n",
      "            \"name\": \"Jinpeng Lan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2145252413\",\n",
      "            \"name\": \"Hanyuan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2188996842\",\n",
      "            \"name\": \"Chenyang Li\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"For each test object, we first apply SAM [29] to extract the object point cloud captured from a single view and then apply Contact-GraspNet [21] to generate 50 grasp pose candidates.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"708a3a56218220277d5b4b55a40a702a9f5078d6\",\n",
      "        \"title\": \"GraspGPT: Leveraging Semantic Knowledge from a Large Language Model for Task-Oriented Grasping\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 32,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2149518385\",\n",
      "            \"name\": \"Chao Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115747796\",\n",
      "            \"name\": \"Dehao Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2148232210\",\n",
      "            \"name\": \"Wenqiang Ge\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109218468\",\n",
      "            \"name\": \"Weiyu Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2210252356\",\n",
      "            \"name\": \"Hong Zhang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"6da92ec389c56ed6937059db90c54d031f70d308\",\n",
      "        \"title\": \"Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 44,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"14571215\",\n",
      "            \"name\": \"A. Jaus\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"151396207\",\n",
      "            \"name\": \"Constantin Seibold\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2224850515\",\n",
      "            \"name\": \"Kelsey Hermann\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2212895137\",\n",
      "            \"name\": \"Alexandra Walter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3120696\",\n",
      "            \"name\": \"K. Giske\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2089190420\",\n",
      "            \"name\": \"J. Haubold\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239665\",\n",
      "            \"name\": \"J. Kleesiek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1742325\",\n",
      "            \"name\": \"R. Stiefelhagen\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"449f8f819c17086efa5c8325bc107d7a38aaed1d\",\n",
      "        \"title\": \"Moving beyond the desktop: prospects for practical bioimage analysis via the web\",\n",
      "        \"venue\": \"Frontiers Bioinform.\",\n",
      "        \"referenceCount\": 55,\n",
      "        \"citationCount\": 1,\n",
      "        \"publicationDate\": \"2023-07-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2066307549\",\n",
      "            \"name\": \"W. Ouyang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152117959\",\n",
      "            \"name\": \"K. Eliceiri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6648707\",\n",
      "            \"name\": \"B. Cimini\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"584ca135b61482fd89247113da87d784f738dbfa\",\n",
      "        \"title\": \"Foundational Models Defining a New Era in Vision: A Survey and Outlook\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 365,\n",
      "        \"citationCount\": 3,\n",
      "        \"publicationDate\": \"2023-07-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"144987292\",\n",
      "            \"name\": \"Muhammad Awais\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40894826\",\n",
      "            \"name\": \"Muzammal Naseer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2111180748\",\n",
      "            \"name\": \"Salman Siddique Khan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3288214\",\n",
      "            \"name\": \"R. Anwer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2951229\",\n",
      "            \"name\": \"Hisham Cholakkal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2111863589\",\n",
      "            \"name\": \"M. Shah\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152790163\",\n",
      "            \"name\": \"Ming Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2358803\",\n",
      "            \"name\": \"F. Khan\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"308d0977addd1620929ba5c63797e4760a721308\",\n",
      "        \"title\": \"Industrial Segment Anything - a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 169,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1720851157\",\n",
      "            \"name\": \"Keno Moenck\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150036040\",\n",
      "            \"name\": \"Arne Wendt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2226283181\",\n",
      "            \"name\": \"Philipp Pr\\u00fcnte\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2054454409\",\n",
      "            \"name\": \"Julian Koch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221120120\",\n",
      "            \"name\": \"Arne Sahrhage\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2142350087\",\n",
      "            \"name\": \"Johann Gierecker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2136155773\",\n",
      "            \"name\": \"Ole Schmedemann\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"117607636\",\n",
      "            \"name\": \"Falko K\\u00e4hler\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2142404142\",\n",
      "            \"name\": \"Dirk Holst\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2137075937\",\n",
      "            \"name\": \"Martin Gomse\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2148495878\",\n",
      "            \"name\": \"Thorsten Sch\\u00fcppstuhl\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1477287242\",\n",
      "            \"name\": \"Daniel Schoepflin\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"[11] and SAM [12] and a library of action primitives.\",\n",
      "        \"Our vision system attempts to segment the foods we wish to cut using the powerful pre-trained segmentation model SAM (Segment Anything Model) by Meta Research team [12].\",\n",
      "        \"More recently, SAM [12] demonstrates a remarkable ability to provide detailed segmentation of cluttered scenes.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\",\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"3e117ec196645c8a103013132a98e6348bc5bb6f\",\n",
      "        \"title\": \"RoboChop: Autonomous Framework for Fruit and Vegetable Chopping Leveraging Foundational Models\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 36,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2224846836\",\n",
      "            \"name\": \"Atharva Dikshit\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2185508465\",\n",
      "            \"name\": \"Alison Bartsch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2186052477\",\n",
      "            \"name\": \"Abraham George\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3614493\",\n",
      "            \"name\": \"A. Farimani\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Using the BodyPix masks as bounding boxes, we apply the Segment Anything Model (SAM)[Kirillov et al. 2023] to generate body segmentation from AIgenerated images, which meet the standards of the game industry.\",\n",
      "        \"Powerful general segmentation model such as Segment Anything Model (SAM)[Kirillov et al. 2023] introduces a novel of zero-shot automatic mask generation; however, it generates masks without considering body structure and semantic context.\",\n",
      "        \"In Figure 3, our pipeline sends the semantic prompt to RegionClip[Zhong et al. 2021], where the bounding box is segmented, using SAM, into an image and position that can be incorporated into the existing workflow.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"b811523aab82909bdc8b09b8da32eb4eaa986363\",\n",
      "        \"title\": \"SegAnimeChara: Segmenting Anime Characters Generated by AI\",\n",
      "        \"venue\": \"SIGGRAPH Posters\",\n",
      "        \"referenceCount\": 8,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2055930071\",\n",
      "            \"name\": \"A. Tseng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2223093945\",\n",
      "            \"name\": \"Wen-Fan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1733344\",\n",
      "            \"name\": \"Bing-Yu Chen\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"13fd1cca96b570c7555c8380b9f964124c36b744\",\n",
      "        \"title\": \"Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 67,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2146393919\",\n",
      "            \"name\": \"Jiancang Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158472093\",\n",
      "            \"name\": \"Junhao Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2141809694\",\n",
      "            \"name\": \"Chen Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2130373\",\n",
      "            \"name\": \"H. Lu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": true,\n",
      "      \"contexts\": [\n",
      "        \"Since a point within the object region is provided as input, we use SAM [38] to generate a segmentation mask.\",\n",
      "        \"The detected bounding box is fed into SAM to generate the object segmentation mask which is then used for inpainting similar to the Removal baseline.\",\n",
      "        \"If no such mask is found, we attempt to get a mask by directly using the point as input to SAM to get a mask.\",\n",
      "        \"So, we first localize the object using OWL-ViT, crop the localized region, and segment it using SAM to create the appropriate input for Zero-1-to-3 for performing the rotation.\",\n",
      "        \"We first use SAM in the generation mode to get candidate masks for the entire scene and select the mask which contains the point and occupies no more than a third of the area of the entire image.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"cae42482b937b087938b7919798b201428780aa3\",\n",
      "        \"title\": \"OBJECT 3DIT: Language-guided 3D-aware Image Editing\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 81,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2196005933\",\n",
      "            \"name\": \"Oscar Michel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35986726\",\n",
      "            \"name\": \"Anand Bhattad\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1632920625\",\n",
      "            \"name\": \"Eli VanderBilt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145237361\",\n",
      "            \"name\": \"Ranjay Krishna\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2684226\",\n",
      "            \"name\": \"Aniruddha Kembhavi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1911972\",\n",
      "            \"name\": \"Tanmay Gupta\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"SAM [35], CLIP [5], Flamingo [36], and DALL-E [37] achieve strong performance in cross-modal matching, crossmodal reasoning, and cross-modal generation.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"0200268766cb1562c676086f331a8e1d8949c38f\",\n",
      "        \"title\": \"Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 72,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2111830888\",\n",
      "            \"name\": \"Weidong Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2667341\",\n",
      "            \"name\": \"Xiaofen Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158502526\",\n",
      "            \"name\": \"Peihao Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9303726\",\n",
      "            \"name\": \"Xiangmin Xu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Over the past decade, significant progress has been made in deep learning, giving rise to a succession of impressive advancements in both large language models [1]\\u2013[3] and large vision models [4]\\u2013[6].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"41abc411fcdeae260bdc78788fade50c54bb97b3\",\n",
      "        \"title\": \"Boosting Federated Learning Convergence with Prototype Regularization\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 15,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2209185953\",\n",
      "            \"name\": \"Yu Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064731560\",\n",
      "            \"name\": \"Huy Q. Le\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159807650\",\n",
      "            \"name\": \"Choong-Seon Hong\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"c66f8bf0c79246693774d85dbbab49eb61c8f59f\",\n",
      "        \"title\": \"Interactive Segmentation for Diverse Gesture Types Without Context\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 67,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1567928285\",\n",
      "            \"name\": \"Josh Myers-Dean\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2193166107\",\n",
      "            \"name\": \"Yifei Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31844147\",\n",
      "            \"name\": \"Brian L. Price\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2224017069\",\n",
      "            \"name\": \"Wilson Chan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2028946\",\n",
      "            \"name\": \"D. Gurari\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Notably, compared to SAM-Adapter [54], which employs SAM [59] with strong generalization ability as the foundation model, our method still represent apparent superiority.\",\n",
      "        \"Notably, compared to SAM-Adapter [54], which employs SAM [59] with strong generalization ability as the foundation\",\n",
      "        \"We compare our method with 16 recent stateof-the-art methods: SINet [1], PraNet [48], TINet [21], ERRNet [49], PFNet [24], UGTR [50], C2FNet [51], SINetV2 [52], S-MGL [2], R-MGL [2], LSR [47], JCSOD [36], ZoomNet [3], BASNet [23], HitNet [53] and SAM-Adapter [54].\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"result\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"7185e9a5904e23e8769e1a70f57e350e19367404\",\n",
      "        \"title\": \"Pre-train, Adapt and Detect: Multi-Task Adapter Tuning for Camouflaged Object Detection\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 78,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"31211187\",\n",
      "            \"name\": \"Yinghui Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2090309257\",\n",
      "            \"name\": \"Dexuan Kong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2223558\",\n",
      "            \"name\": \"Shizhou Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155475183\",\n",
      "            \"name\": \"Geng Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2769063\",\n",
      "            \"name\": \"Lingyan Ran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155302795\",\n",
      "            \"name\": \"Peng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2129514046\",\n",
      "            \"name\": \"Yanning Zhang\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"6a128b1d9d8c84c8d78f1bc02a4b7f68ce086b1d\",\n",
      "        \"title\": \"CNOS: A Strong Baseline for CAD-based Novel Object Segmentation\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 28,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2160941377\",\n",
      "            \"name\": \"Van Nguyen Nguyen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2396902\",\n",
      "            \"name\": \"Tom\\u00e1s Hodan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"103081517\",\n",
      "            \"name\": \"G. Ponimatkin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35930660\",\n",
      "            \"name\": \"Thibault Groueix\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1689738\",\n",
      "            \"name\": \"V. Lepetit\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"The separation did not change until the release of CLIP [11], and recently took a stride forward with the emergence of segment anything model [12] and VisionLLM [13].\",\n",
      "        \", SAM [12] and VisionLLM [13]) raises more attentions on multimodal VL learning, which generally regards language as a prompt to instruct downstream tasks.\",\n",
      "        \"Recently, the emergence of large visual foundation models (e.g., SAM [12] and VisionLLM [13]) raises more attentions on multimodal VL learning, which generally regards language as a prompt to instruct downstream tasks.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"background\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"f30e1f48703bb319222e005ea7b6b8ab1b9e035e\",\n",
      "        \"title\": \"Divert More Attention to Vision-Language Object Tracking\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 79,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2151671001\",\n",
      "            \"name\": \"Mingzhe Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1560026463\",\n",
      "            \"name\": \"Zhipeng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50780902\",\n",
      "            \"name\": \"Li Jing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1805398\",\n",
      "            \"name\": \"Haibin Ling\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143911163\",\n",
      "            \"name\": \"Heng Fan\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"1b4012f38daa8f09299e16771973c91ce9464ee2\",\n",
      "        \"title\": \"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 54,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"153245407\",\n",
      "            \"name\": \"Along He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"37833805\",\n",
      "            \"name\": \"Kai Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2216099246\",\n",
      "            \"name\": \"Zhihong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1840553660\",\n",
      "            \"name\": \"Tao Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1929093\",\n",
      "            \"name\": \"H. Fu\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"1eb28386605bad387a3315e18eda6ea9dd5d7877\",\n",
      "        \"title\": \"Opportunities and challenges for deep learning in cell dynamics research\",\n",
      "        \"venue\": \"\",\n",
      "        \"referenceCount\": 181,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"30747783\",\n",
      "            \"name\": \"Binghao Chai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2123710199\",\n",
      "            \"name\": \"Christoforos Efstathiou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2210849469\",\n",
      "            \"name\": \"Haoran Yue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4708683\",\n",
      "            \"name\": \"V. Draviam\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [],\n",
      "      \"intents\": [],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"8ef83aec0d772ebb9fb22e49d15aa979d38d3ef4\",\n",
      "        \"title\": \"PharmacyGPT: The AI Pharmacist\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 74,\n",
      "        \"citationCount\": 1,\n",
      "        \"publicationDate\": \"2023-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2145977326\",\n",
      "            \"name\": \"Zheng Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47039788\",\n",
      "            \"name\": \"Zihao Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2215174877\",\n",
      "            \"name\": \"Mengxuan Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112525162\",\n",
      "            \"name\": \"Bokai Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2111641126\",\n",
      "            \"name\": \"Lin Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221113406\",\n",
      "            \"name\": \"Tianyi Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"29944950\",\n",
      "            \"name\": \"Haixing Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2167589230\",\n",
      "            \"name\": \"Xianyan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2196205696\",\n",
      "            \"name\": \"Ye Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153702893\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2156692365\",\n",
      "            \"name\": \"Brian Murray\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115345993\",\n",
      "            \"name\": \"Tianming Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"84406057\",\n",
      "            \"name\": \"A. Sikora\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"isInfluential\": false,\n",
      "      \"contexts\": [\n",
      "        \"Off-the-shelf segmentation tool [48] and depth estimator [49] are used to generate the object masks and depth images of the ImageNet [11] and AI generated [10] images.\"\n",
      "      ],\n",
      "      \"intents\": [\n",
      "        \"methodology\"\n",
      "      ],\n",
      "      \"citingPaper\": {\n",
      "        \"paperId\": \"2f1de06684c1d8609727cf3a60aa4ebb7f090ab4\",\n",
      "        \"title\": \"NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF\",\n",
      "        \"venue\": \"arXiv.org\",\n",
      "        \"referenceCount\": 52,\n",
      "        \"citationCount\": 0,\n",
      "        \"publicationDate\": \"2023-07-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2008683494\",\n",
      "            \"name\": \"S. Lionar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48669892\",\n",
      "            \"name\": \"Xiangyu Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"119732088\",\n",
      "            \"name\": \"Min-Hui Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110775592\",\n",
      "            \"name\": \"G. Lee\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    f'https://api.semanticscholar.org/graph/v1/paper/{\"7470a1702c8c86e6f28d32cfa315381150102f5b\"}/citations',\n",
    "    params={'fields': 'contexts,intents,title,isInfluential,paperId,venue,authors,referenceCount,citationCount,publicationDate,influentialCitationCount'},\n",
    ")\n",
    "print(json.dumps(r.json(), indent=2))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T11:20:50.461003400Z",
     "start_time": "2023-08-29T11:20:49.512625200Z"
    }
   },
   "id": "7bb1b7aa6d64fe38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "709ec65325061e50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
