{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T08:47:05.731742800Z",
     "start_time": "2023-08-17T08:47:05.605572600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from arxiv import SortCriterion, SortOrder\n",
    "import arxiv\n",
    "\n",
    "query = f'Instruction-vit: Multi-modal prompts for instruction learning in vit'\n",
    "search = arxiv.Search(query=query,\n",
    "                      max_results=float('inf'),\n",
    "                      sort_by=SortCriterion.Relevance,\n",
    "                      sort_order=SortOrder.Descending,\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ddfd66dd0266ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T08:47:08.034719100Z",
     "start_time": "2023-08-17T08:47:05.726743Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rst = next(search.results())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb84350e607c6758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T08:48:15.253092600Z",
     "start_time": "2023-08-17T08:48:15.233065Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_attributes(obj):\n",
    "    attributes = vars(obj)\n",
    "    for attr, value in attributes.items():\n",
    "        print(f\"{attr}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134898612d36e312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T08:48:21.301335800Z",
     "start_time": "2023-08-17T08:48:21.265311800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_id: http://arxiv.org/abs/2305.00201v1\n",
      "updated: 2023-04-29 08:59:12+00:00\n",
      "published: 2023-04-29 08:59:12+00:00\n",
      "title: Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT\n",
      "authors: [arxiv.Result.Author('Zhenxiang Xiao'), arxiv.Result.Author('Yuzhong Chen'), arxiv.Result.Author('Lu Zhang'), arxiv.Result.Author('Junjie Yao'), arxiv.Result.Author('Zihao Wu'), arxiv.Result.Author('Xiaowei Yu'), arxiv.Result.Author('Yi Pan'), arxiv.Result.Author('Lin Zhao'), arxiv.Result.Author('Chong Ma'), arxiv.Result.Author('Xinyu Liu'), arxiv.Result.Author('Wei Liu'), arxiv.Result.Author('Xiang Li'), arxiv.Result.Author('Yixuan Yuan'), arxiv.Result.Author('Dinggang Shen'), arxiv.Result.Author('Dajiang Zhu'), arxiv.Result.Author('Tianming Liu'), arxiv.Result.Author('Xi Jiang')]\n",
      "summary: Prompts have been proven to play a crucial role in large language models, and\n",
      "in recent years, vision models have also been using prompts to improve\n",
      "scalability for multiple downstream tasks. In this paper, we focus on adapting\n",
      "prompt design based on instruction tuning into a visual transformer model for\n",
      "image classification which we called Instruction-ViT. The key idea is to\n",
      "implement multi-modal prompts (text or image prompt) related to category\n",
      "information to guide the fine-tuning of the model. Based on the experiments of\n",
      "several image captionining tasks, the performance and domain adaptability were\n",
      "improved. Our work provided an innovative strategy to fuse multi-modal prompts\n",
      "with better performance and faster adaptability for visual classification\n",
      "models.\n",
      "comment: None\n",
      "journal_ref: None\n",
      "doi: None\n",
      "primary_category: cs.CV\n",
      "categories: ['cs.CV']\n",
      "links: [arxiv.Result.Link('http://arxiv.org/abs/2305.00201v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.00201v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url: http://arxiv.org/pdf/2305.00201v1\n",
      "_raw: {'id': 'http://arxiv.org/abs/2305.00201v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.00201v1', 'updated': '2023-04-29T08:59:12Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=4, tm_mday=29, tm_hour=8, tm_min=59, tm_sec=12, tm_wday=5, tm_yday=119, tm_isdst=0), 'published': '2023-04-29T08:59:12Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=4, tm_mday=29, tm_hour=8, tm_min=59, tm_sec=12, tm_wday=5, tm_yday=119, tm_isdst=0), 'title': 'Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=Instruction-vit%3A+Multi-modal+prompts+for+instruction+learning+in+vit&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100', 'value': 'Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT'}, 'summary': 'Prompts have been proven to play a crucial role in large language models, and\\nin recent years, vision models have also been using prompts to improve\\nscalability for multiple downstream tasks. In this paper, we focus on adapting\\nprompt design based on instruction tuning into a visual transformer model for\\nimage classification which we called Instruction-ViT. The key idea is to\\nimplement multi-modal prompts (text or image prompt) related to category\\ninformation to guide the fine-tuning of the model. Based on the experiments of\\nseveral image captionining tasks, the performance and domain adaptability were\\nimproved. Our work provided an innovative strategy to fuse multi-modal prompts\\nwith better performance and faster adaptability for visual classification\\nmodels.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'http://export.arxiv.org/api/query?search_query=Instruction-vit%3A+Multi-modal+prompts+for+instruction+learning+in+vit&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100', 'value': 'Prompts have been proven to play a crucial role in large language models, and\\nin recent years, vision models have also been using prompts to improve\\nscalability for multiple downstream tasks. In this paper, we focus on adapting\\nprompt design based on instruction tuning into a visual transformer model for\\nimage classification which we called Instruction-ViT. The key idea is to\\nimplement multi-modal prompts (text or image prompt) related to category\\ninformation to guide the fine-tuning of the model. Based on the experiments of\\nseveral image captionining tasks, the performance and domain adaptability were\\nimproved. Our work provided an innovative strategy to fuse multi-modal prompts\\nwith better performance and faster adaptability for visual classification\\nmodels.'}, 'authors': [{'name': 'Zhenxiang Xiao'}, {'name': 'Yuzhong Chen'}, {'name': 'Lu Zhang'}, {'name': 'Junjie Yao'}, {'name': 'Zihao Wu'}, {'name': 'Xiaowei Yu'}, {'name': 'Yi Pan'}, {'name': 'Lin Zhao'}, {'name': 'Chong Ma'}, {'name': 'Xinyu Liu'}, {'name': 'Wei Liu'}, {'name': 'Xiang Li'}, {'name': 'Yixuan Yuan'}, {'name': 'Dinggang Shen'}, {'name': 'Dajiang Zhu'}, {'name': 'Tianming Liu'}, {'name': 'Xi Jiang'}], 'author_detail': {'name': 'Xi Jiang'}, 'author': 'Xi Jiang', 'links': [{'href': 'http://arxiv.org/abs/2305.00201v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.00201v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n"
     ]
    }
   ],
   "source": [
    "print_attributes(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ecb6475cbc08a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
